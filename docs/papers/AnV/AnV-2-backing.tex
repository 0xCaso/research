
\section{Availability and backing}
\label{sec:availability_n_backing}
% \label{sec:backing}

We now describe the subprotocols that provide the preliminary financial backing checks from parachain validators and that ensure availability for our later approval validity checks.


We now describe and instantiate our {\em availability and validity protocol} that provides efficient sharding.  It consists of 
\begin{itemize}
\item a parachain phase that prepares the candidate block and performs preliminary backing validity checks,
\item a relay chain submission phase that distributes candidate parachain blocks and produces relay chain blocks,
\item availability and unavailability subprotocols that enforce availability in GRANDPA and BABE respectively,
\item fuller approval validity checks for GRANDPA,
\item objection procedures for fishermen, and
\item invokation of a Byzantine fault tolerant ``finality gadget'' that gives us finality.
\end{itemize}


% TODO: Anything more worth saying here?  Maybe extract from:  The parachain phase is executed between collators and parachain validators. In the end of this phase, the parachain validators validate the block and provide its erasure code pieces to the validators. Then, the relay chain phase begins. If the parachain phase is executed correctly, then the relay chain phase includes extra validation of a parachain block, adding the block header to the relay chain and finalizing that relay chain block. Otherwise, unavailability protocol is run between validators. The details are as follows: 


\subsection{Parachain phase} 
\label{sec:parachain}

We first describe the protocol by which collators of a parachain $\para$ submit a candidate block to the parachain validators assigned to $\vals_\para$.

\smallskip
% \paragraph{Collator subphase:} 

Initially, a {\bf collator} $C$ of a parachain $\para$ must propose some candidate block $B$ for $\para$.  We let $B'$ denotes the parent of $B$.  As above, let $\rin$ and $\rout$ denote state root before and after executing $B$.  

In practice, we want shared security so that parachains can communicate, so $B$ should reference some relay chain block(s) $R^0_B$ that distinguish any state $\rho$ maintains on the relay chain, such as the incoming messages accumulated for $\rho$ to be processed by $B$.  We let $q$ denote the Merkle root of this state $\rho$ maintains on the relay chain in $R^0_B$.
 
First, $C$ constructs the witness data $\pi$ by evaluating the block with $\prove_{\hat{\para}}(B,M)$, so they can build the {\em candidate proof-of-validity blob} $\blobB = (B,\pi,M)$, and also obtain the block metadata $(H(B'),\rin,\rout,H(R^0_B),q,\ldots)$. 

As $\prove$ is a randomized algorithm, $C$ must next reevaluate the block with $\verify_{\hat{\para}}(\blobB)$.  We shall assume verification succeeds, but if this verification fails then $C$ reports invalid parachain code for $\para$, and discards $B$ or possibly shuts down.  Assuming no errors, $C$ sends the candidate blob $\blobB$ to the corresponding parachain validators $\vals_\para$, along with any block metadata $(H(B'),\rin,\rout,H(R^0_B),q,\ldots)$. 

\smallskip
% \paragraph{Parachain validator:}

We shall expect the parachain validator $V$ that processes candidate blob $\blobB$ to update $\blobB$'s associated metadata to reference the most recent relay chain block $R^1_B > R^0_B$, so long as $R^1_B$ still contains the same state root $q$ for $\rho$,
i.e.\ $(H(B'),\rin,\rout,H(R^1_B),$\vect{R^1_B q}$,\ldots)$.

Next each {\em parachain validator} $V \in \vals_\para$ should checks the validity of the block by evaluating the block with $\verify_\para(\blobB)$, which we term the {\em preliminary backing validity checks}.  If verification succeeds, then $V$ gossips $\blobB$ among the parachain validators $\vals_\para$ after checking $\blobB$ itself, and we continue below.

If however verification fails, the parachain validator rejects the candidate $\blobB$ and report it as invalid.  We abandon $B$ if no validators sign it, but invalidity claims cannot necessarily result in penalties for either $\para$ or $C$.  

At any time, if any two validators disagree about a parachain block's validity then all validators shall check the block.  In this case, we accumulate votes until $f+1$ claim validity or invalidity, and then slash the loosing side.  We cannot slash if neither side reaches $f+1$, but we still declare the block invalid in that case.  We expect governance to identify software faults and manually revert slashes they cause, but governance can also manually institute slashes in this second case, or manually slash $\para$ for offenses like malicious code or improper non-determinism. 


\subsection{Relay chain authorship} % Relay chain phase I: Block production 
\label{sec:backing}

% \smallskip
% \paragraph{Parachain validator:}

We require at least $\primarychecks$ of the parachain validator $\vals_\para$ sign preliminary backing validity checks for $B$ before continuing, so we assume this going forward.

Now any {\em parachain validator} $V \in \vals_\para$ runs $\encode_{f+1,\nvals}(\blobB)$ to obtain the {\em unauthenticated pieces} list $\prepieces_B$ of $\nvals$ erasure code symbols aka pieces of $\blobB$.  Next $V$ computes a Merkle root $\merkleroot_B$ for the Merkle tree with leaves $\prepieces_B$.  $V$ constructs the attested candidate receipt $\receipt_{B,\{(V,\sigma)\}} := (\receipt_{B},\{V\})$ for $B$ by signing an (inner) candidate receipt $\receipt_B = (\para.\mathsf{id},H(\receipt_{B'}),\merkleroot_B,H(B),\rin,\rout)$ where $B'$ denotes the parent of $B$, and attaching its signature\handan{}{ and where id is ...}.  
% TODO: Improve explicit state root commitments maybe?
% Also Joe asks if we should talk about parachain block headers
% Anything else?

We gossip these attested candidate receipts $\receipt_{B,S}$ among the parachain validators $\vals_\para$.  
%
In doing so, we improve them by further aggregating the signature set $S$.\footnote{We envision $\vals_\para$ being small enough that BLS signatures do not improve verification time over Schnorr signatures, although BLS might reduce the candidate receipt's signature from 640 bytes down to 50 bytes.}  We publish $\receipt_{B,S}$ for relay chain block producers using relay chain gossip (mempool) whenever $S$ restricted to $\vals_\rho$ has attestations from at least $\primarychecks$ validators, assuming this happens eventually.  If $S$ remains too small, then validators in $\vals \setminus \vals_\rho$ ignore $\receipt_{B,S}$.
We think $\primarychecks = {1\over2} \npvals$ gives a reasonable choice, but our security analysis only requires enough stake back $B$, so $\primarychecks = 1$ suffices provided the backing validator possesses enough stake.

We archive $B$ if another conflicting blocks gets finalised by GRANDPA, eventually deleting it.  We archive but probably do not delete $B$ if GRANDPA is stalled, probably even when the fork choice rule favours other forks.  

% \smallskip
% \paragraph{Relay chain block producer:}

Any {\bf relay chain block producer} $U \in \vals$ with an upcoming relay chain block production slot enqueues any attested candidate receipts $\receipt_{B,S}$ with sufficient $S$ received for possible inclusion in some future relay chain block that $U$ creates.  

We now suppose $U$ makes a relay chain block $R$ that includes $\receipt_{B,S}$.  We of course need $R$ to have an ancestor $R'$ that includes the candidate receipts $\receipt_{B',S'}$ for the parent parachain block $B'$ of our candidate $\receipt_{B,S}$, and that no $R''$ between $R$ and $R'$ include any block from $\para$.  Ideally $U$ continues collecting the signatures $S$ on $\receipt_{B,S}$ while waiting its turn too.  See \href{http://research.web3.foundation/en/latest/polkadot/BABE/Babe/}{BABE} for more details on block production.
% TODO: Any specific comments on relay chain block headers $\bh$

Associated to each unauthenticated piece $d \in \prepieces_B$, we define an {\em authenticated (erasure coded candidate) piece} $(d,\merkleroot_B,\vect{\merkleroot_B d})$ by attaching the Merkle root $\merkleroot_B$ and an inclusion proof $\vect{rd}$, which authenticates $d$ as being committed to by the Merkle root $\merkleroot_B$.  Of course this expansion commutes with expanding the signer set $S$ on the attested candidate receipt $\receipt_{B,S}$.

We handle only the list $\pieces_B$ of these authenticated pieces for the remainder of the protocol. 
$$ \pieces_B = Listst{ (d,\receipt_{B,S},\vect{\merkleroot_B d}) }{ d \in \prepieces_B } $$

We must distribute $\pieces_B$ among the full relay chain validator set $\vals$ with $\pieces_B[i]$ going to $\vals[i]$ for $i = 1,\ldots,\nvals$.  In so doing. we force the signer set $S$ into making $\blobB$ available for testing by random approval checkers without yet revealing those approval checkers.  This trick provides the core scalability advantage of Polkadot.

We might however see many competing parachain candidate blocks at this point, so we delay this distribution process until some relay chain block $R$ contains $\receipt_{B,S}$.  We assume such an $R$ containing $\receipt_{B,S}$ exists throughout the remainder of this section.


\subsection{Topology}
\label{sec:topology}
%TODO: "Piece distribution topology" is too long

We find that scalability actually depends heavily upon the topology and routing used to distribute data, but that specifics depend upon the scale.  We briefly explain the specialised topology and routing requirements for our two phases of parachain data distribution.  We caution however that routing almost always requires some capacity for multi-hop forwarding because otherwise risks excluding some elected validators.  

\smallskip
\paragraph{Candidate blocks:}

We need our network topology to permit one parachain $\para$ to distribute the authenticated erasure coded candidate pieces $\pieces_B$s relatively quickly, which amounts to a graph expansion property.  We might want a much stronger connectivity property below if doing full reconstructions turns out to be our most efficient mechanism for block retrieval, but if not then some similar expansion property permits forwarding pieces for reconstructions (see \S\ref{sec:retrieval}).

We might also ask good collators to send the same block to several well chosen parachain validators as well.  As an example, if our parachain validators $\vals_\para$ form a cycle then $B$ reaches all parachain validators in two hops if the collators send $B$ to one third of $\vals_\para$, but adding well chosen chords reduces our connections with collators and improves our connectivity.  

\smallskip
\paragraph{Candidate pieces:}

All parachain validators in $\vals_\para$ must compute all of $\prepieces_B$ to compute $\merkleroot_B$, from which computing $\pieces_B$ too costs nothing.  We should therefore divide the distribution burden as equally as possible among parachain validators in $\vals_\para$.  We also prefer if the topology is symmetric in the sense that the links over which $\para_1$ sends to validators in $\para_2$ are the same as the links over which $\para_2$ sends to validators in $\para_1$.  We now outline an extremely simple topology that satisfies these requirement:

We recall any parachain $\para_i$ is equipped with a somewhat ephemeral value $\para_i.\mathsf{seed}$ that depends upon its parachain validator assignment and some on-chain randomness $r$.
$$ \para_i.\mathsf{seed} := H\left( r, \mathsf{sort} \setst{ V.\mathsf{pk} }{ V \in \vals_{\para_i} } \right) $$

We begin by assuming some abstract symmetric topology $\mathcal{T}$ on the set parachains, preferably a complete graph, i.e.\ diameter one.  Consider two parachains $\para_1$ and $\para_2$ with disjoint validator sets that are connected by an edge in $\mathcal{T}$.  We let $\mathsf{parashuffle}(\para_i,\para_{2-i})$ denote the Fisher-Yates shuffle of $\vals_{\para_i}$ seeded by $H( \para_i.\mathsf{seed}, \para_{2-i}.\mathsf{seed} )$.  We now define a topology $\mathcal{T}_e$ on the $\vals$ by connecting $\mathsf{parashuffle}(\para_1,\para_2)[j]$ to $\mathsf{parashuffle}(\para_2,\para_1)[j]$ for $j = 1,\ldots,\npvals$.

We can adapt this scheme to varying $|\vals_{\para_i}| \ge \npvals$ quite easily if not all parachains have the same number of assigned validators, i.e.\ if $\npvals$ is not tight.  We can also do additional shuffles if more than one link is desired.  
% 
If assigned two nodes cannot connect to one another, then any still online attempt connections with some random other nodes from the other parachain, or perhaps use some smarter scheme. 

As an example, assume $\mathcal{T}$ is a complete graph:  After our parachain validator $V$ of $\para$ observes some relay chain block $R$ containing $\receipt_{B,S}$ then, for all other parachains $\para' \ne \para$, $V$ computes the $i_{\para'}$s such that $V = \mathsf{parashuffle}(\para,\para')[j]$ and $\vals[i_{\para'}] = \mathsf{parashuffle}(\para',\para)[j]$ for some $j \leq \npvals$, and $V$ send $\pieces_B[i_{\para'}]$ to $\vals[i_{\para'}]$ directly using QUIC.  We expect $\vals[i_{\para'}]$ might have some piece from $\para'$ for $V$ too, thanks to the symmetry of our $\mathsf{parashuffle}$ criteria.  We expect this symmetry to reduce the required connections by almost a factor of two.

We have described this as $V$ initiating the connection, but a similar procedure works for $V$ requesting its piece for some $\para'$ block, or symmetrically $\vals[i_{\para'}]$ requesting $\pieces_B[i_{\para'}]$.  In fact, an initial implementation should focus upon requests because as noted above we shall request from other validators when our first choice fails. 

If $\vals[i_{\para'}]$ cannot reach $V$ then $\vals[i_{\para'}]$ must select some backup node to replace $V$.  Assuming $\mathcal{T}$ is complete, we should distribute these evenly among $\vals_\para \setminus \{V\}$, so as one option $\vals[i_{\para'}]$ could perform a Fisher-Yates shuffle of $\vals_\para \setminus \{V\}$, seeded by its own identity $\vals[i_{\para'}].\mathsf{pk}$ and $\para.\mathsf{seed}$, and then contact those remaining parachain validators in the resulting order.  We caution this option breaks the topology's symmetry, so as noted above nodes might exploit whatever links work first, and only take guidance from this shuffle when creating new links.  If $\mathcal{T}$ is not complete then alternative approaches that choose another parachain work too.  

If $\mathcal{T}$ is not complete then intermediate nodes must forward authenticated pieces for other nodes.  In fact, the diameter of $\mathcal{T}$ equals the maximum number of hops required for $\mathcal{T}_e$ to distribute each piece.  
In practice, these edges in $\mathcal{T}_e$ ro be the two nodes
% $\mathsf{parashuffle}(\para_1,\para_2)[j]$ to $\mathsf{parashuffle}(\para_2,\para_1)[j]$
maintaining a UDP protocol like QUIC connection because UDP should permit higher valency than TCP and hence permit a lower diameter $\mathcal{T}$.  
We should evaluate other topologies besides $\mathcal{T}_e$ before going beyond complete $\mathcal{T}$ proves necessary, but our symmetry property provided by $\mathcal{T}_e$ remains important. 

We admitted adversarial manipulation of our network topology here, but it sounds acceptable for our availability scheme, at least with $\mathcal{T}$ complete.  We shall consider whether this impacts gossip protocols in future work. 
% TODO:  Future work?  Here below?


\subsection{Availability}
\label{sec:availability}  % Pre-GRANDPA

% We isolated work on candidates for any given parachain $\para$ to $\vals_\para$ ...
At this point, we consider a relay chain block $R$ being gossiped among the validators $\vals$.  $R$ contains candidate receipts $\receipt_{B,S}$ for candidate blocks $B$ on distinct parachains $\para$.  

We must established that our authenticated pieces $\pieces_B$ for each $B$ in $R$ were correctly distributed before our approval checkers announce their assignments in \S\ref{sec:assignment} or run their assigned checks in \S\ref{sec:approcal_checks}, which must happen before our finality gadget GRANDPA considers $R$ in \S\ref{sec:finality}.  

As our erasure coding permits recovery from any $f+1$ pieces, we establish correct distribution simply by asking all validators to ``vote'' whether they received their assigned authenticated pieces $\pieces_B$ for all the parachain candidates $B$ in $R$. 
% which makes $R$ {\em available} (see \S\ref{sec:approval}).

It suffices if validators announce an availability attestation via gossip once they receive their authenticated piece $\pieces_B$ for each $B$ in $R$.  In this design, a validator $V$ considers a relay chain block $R$ to be {\em strongly available} once it observes $f+1$ availability attestations for $R$ and possesses all it own pieces for candidates in $R$.

As an alternative, we could ask that validators gossip availability announcements for each candidate receipt $\receipt_{B,S}$ separately.  In this design, we achieve a weaker ``mixed'' availability notion more quickly, which itself suffices to begin the approval checks in \S\ref{sec:approval}.  We need more bandwidth and computation fort his approach, although this could be reduced if availability attestation simply contained a bitfield or other compressed scheme for selecting parachains.

We should require each validator possess its authenticated pieces from $\pieces_B$ before voting in our finality gadget too though, which imposes our first stronger availability notion before finality anyways.  In other words, we only profit from the weaker ``mixed'' availability notion by launching approval validity checks sooner.

In either case, all validators establish an availability notion for relay chain blocks long before any GRANDPA votes, which triggers the later approval checks discussed blow in \S\ref{sec:approval}.


\subsection{Unavailability}
\label{sec:unavailability} % BABE

We cannot entirely escape the availability question within our relay chain block production scheme BABE either:  Imagine we have several forks $C_1,\ldots,C_k$ for which at most $f$ validators possess all their chunks, but no fork for which $f+1$ validators possess all their chunks.  Yet, each block producers $U$ possess all their chunks for at least one fork $C_i$.  If BABE were oblivious to availability, then $U$ extends $C_i$, and GRANDPA stalls under this configuration. 
% TODO:  Anything about secondary validity check here ???

Instead, we define an availability grace period $\grace$ after which an unavailability subprotocol alters BABE's chain selection rule:  

If a validator $U \in \vals$ cannot obtain some piece $d \in \pieces_B$ within $\grace$ time after seeing the attested candidate receipt $\receipt_{B,\cdot}$ included in some relay chain block $R$, then $U$ announces via gossip the unavailability of the piece $d$. 
% TODO: Was $R_{head}$ but really?
In essence, these unavailability announcements are precisely opposite our second piecemeal availability announcement strategy, perhaps replacing the bitfield with trifield represented as 5 trits per bytes, or another compression technique that made availability announcements more efficient than unavailability.

As unavailability announcements are always piecemeal, if we adopt the first availability announcement strategy that covers whole relay chain blocks then we might still need partial availability announcements to revoke unavailability announcements, either for BABE or if not using requests for pieces then to halt send attempts.

We now explain the validators' responses to these unavailability announcements:

First, suppose some validator $V \in \vals$ observes an unavailability announcement for some attested candidate receipt $\receipt_{B,S}$ from some validator $\vals[i]$.  If $V$ possesses the block $\blobB$ then $V$ has computed $\pieces_B$ to check $\receipt_{B,S}$, so $V$ already possesses $\pieces_B[i]$ and should offer it to $\vals[i]$.  Any $V \in S$ satisfies this, but so shall any validators who signed off on the approval checks discussed below in \S\ref{sec:validity}. 

Second, if any validator $U \in \vals$ observes unavailability announcements for pieces of some attested candidate receipt $\receipt_{B,\cdot}$ from at least $f+1$ different validators, then in BABE a block producer $U$ shall not propose a relay chain block containing any $\receipt_{B',\cdot}$ for which $B'$ is a descendent of $B$.
In this situation, there might be prevotes but never any precommits in GRANDPA for chains containing  $\receipt_{B,\cdot}$, so $\receipt_{B,\cdot}$ cannot possibly be finalised by GRANDPA.  We should consider if the GHOST chain weighting rule used by BABE and by GRANDPA for prevotes should weigh unavailability announcements, but doing so should only impact performance.

As an aside, we also considered $U$ abandoning any fork $C$ for which at least $f+1$ different validators gossiped unavailability announcements for possibly distinct blobs in $C$, instead of for the same blob.  Any truly unavailable pieces eventually trigger both conditions but they trigger this variant with possibly distinct blobs first, and with more false positives.  We optimise for the honest case here and caution that more false positives results in more chains, so spammy parachains could create more load on the availability system.

Any validator should revoke their past unavailability announcements for some piece $d \in \pieces_B$ whenever they eventually obtain $d$, again by gossiping the revocation.  We also define some super availability grace period, longer than $\npvals \grace$, after which time, if $2f+1$ of the validators announced unavailability of some specific piece, then the parachain validators who signed for that block are slashed.
We revert this slash whenever revocations later reduce the unavailability announcements below $f+1$.
% TODO: Any interactions with slashing computation?


