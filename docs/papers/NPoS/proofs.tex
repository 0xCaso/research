\section{Computing the maximum score in our heuristic}\label{s:maxscore}

For a fixed partial solution $(A,w)$ and for each candidate $c'\in C\setminus A$, consider the function 
$f_{c'}(t):=prescore_w(c',t)-t$ in the interval $[0,\infty)$. 
Notice from the definition of pre-scores~\eqref{eq:prescore} that this funcion is convex, continuous and strictly decreasing with no lower bound, and that $f_{c'}(0)\geq 0$; hence it has a unique root, which corresponds precisely to $score_w(c')$. We could approximate this root via binary search -- however, we can do better. Function $f_{c'}(t)$ is piece-wise linear: if we sort the member supports $\{supp_w(c): \ c\in A\}=\{t_1, \cdots, t_r\}$ so that $t_1 < \cdots < t_r$ for some $r\leq |A|$, then $f_{c'}(t)$ is linear in each of the intervals $[0, t_1), [t_1, t_2), \cdots, [t_r, \infty)$.

Similarly, function $f(t):= \max_{c'\in C\setminus A} f_{c'}(t) = \max_{c'\in C\setminus A} prescore_w(c',t) -t$ is continuous and strictly decreasing in the interval $[0,\infty)$, with a unique root $t_{\max}=\max_{c'\in C\setminus A} score_w(c')$. Unfortunately, this function is in general not linear within each of the intervals above.%
%
\footnote{In each of these $O(k)$ intervals, function $f(t)$ is piece-wise linear with $O(|C|)$ pieces. We could thus find the root of $f(t)$ via binary search by performing $O(\log k + \log |C|)$ calls to $\maxprescore$. However, our approach only requires $O(\log k)$ such calls.} %
%
Still, it will be convenient to use binary search to identify the interval that contains $t_{\max}$. We do so in Algorithm~\ref{alg:interval}. The next lemma follows from our exposition and its proof is skipped.

\begin{algorithm}[htb]\label{alg:interval}
\SetAlgoLined
\KwData{Partial solution $(A,w)$.}
Sort the member supports to obtain $0=t_0<t_1<\cdots <t_r$, where $\{t_1, \cdots, t_r\}=\{supp_w(c): \ c\in A\}$\;
\lIf{$p_{t_r}\geq t_r$ where $(c_{t_r},p_{t_r})\leftarrow \maxprescore(A,w,t_r)$}{\Return $t_r$}
Let $j_{lo}=0$, $j_{hi}=r-1$\;
\While{$j_{lo}<j_{hi}$}{
  Let $j=\lceil (j_{lo}+j_{hi})/2 \rceil$\;
  \eIf{$p_{t_j}\geq t_j$ where $(c_{t_j},p_{t_j})\leftarrow \maxprescore(A,w,t_j)$}{
  Set $j_{lo}\leftarrow j$\;}{
  Set $j_{hi}\leftarrow j-1$\;}
}
\Return $t_{j_{lo}}$\;

 \caption{$\interval(M,w)$}
\end{algorithm}

\begin{lemma}
For a partial solution $(A,w)$, Algorithm $\interval(A,w)$ makes $O(\log |A|)$ calls to $\maxprescore$, and thus runs in time $O(|E|\cdot \log k)$. It returns a value $t'$ so that $t'\leq t_{\max}:=\max_{c'\in C\setminus A} score_w(c')$, and for each candidate $c'\in C\setminus A$, the value of $prescore_w(c',t)$ is linear in $t$ within the interval $[t',t_{\max}]$.
\end{lemma}

Once we have identified such a value $t'$, we exploit the fact that for each $c'\in C\setminus A$, function $f_{c'}(t)$ is linear inside the interval $[t',t_{\max}]$. If we fix a candidate $c'$ and linearize function $f_{c'}(t)$ by extending its linear behavior within $[t', t_{\max}]$ onto the full domain $[0,\infty)$, and we denote its corresponding unique root by $t_{c'}$, then we have 
%
\begin{align*}
    0&= f_{c'}(t_{c'})|_{\text{linearized around } [t', t_{\max}]}\\
    &=prescore_w(c', t_{c'})|_{\text{linearized around } [t', t_{\max}]} - t_{c'}\\
    &=\sum_{n\in N_{c'}} slack_w(n,t)|_{\text{linearized around } [t', t_{\max}]} - t_{c'}\\
    &=\sum_{n\in N_{c'}} \Big( s_n - \sum_{c\in A\cap C_n: \ supp_{w}(c)< t'}w_{nc} - \sum_{c\in A\cap C_n: \ supp_w(c)\geq t'} w_{nc}\cdot t_{c'}/supp_w(c) \Big) - t_{c'},
\end{align*}
%
where we used the definitions of pre-score and slack. Solving for $t_{c'}$, we obtain
%
\begin{align*}
    t_{c'}=\frac{\sum_{n\in N_{c'}} \Big( s_n - \sum_{c\in A\cap C_n: \ supp_w(c)< t'} w_{nc} \Big)}%
    {1+\sum_{n\in N_{c'}} \sum_{c\in A\cap C_n: \ supp_w(c)\geq t'} \frac{w_{nc}}{supp_w(c)}}=:linscore_w(c',t').
\end{align*}

Notice that the expression above depends only on $t'$ and not on $t_{\max}$. We refer to this value as \emph{the linearized score of $c'$ around $t'$}, and denote it by $linscore_w(c',t')$. For each candidate $c'\in C\setminus A$ we have that, since $f_{c'}(t)$ is a convex decreasing function over $[0,\infty)$ and its linearization around $t'$ is tangential to it, the root of this linearization must be lower than its own root, i.e. $linscore_w(c',t')\leq score_w(c')$. 
On the other hand, for the candidate $c_{\max}$ with highest score $t_{\max}$, these two roots must coincide, i.e. $t_{\max}=score_w(c_{\max})=linscore_w(c_{\max}, t')$. Consequently, $c_{\max}$ also has the highest linearized score around $t'$ among all candidates in $C\setminus A$, and we can exploit this fact to find it. We formalize these observations in Algorithm~\ref{alg:maxscore} and the lemma below.

\begin{algorithm}[htb]\label{alg:maxscore}
\SetAlgoLined
\KwData{Partial solution $(A,w)$.}
Let $t'\leftarrow \interval(A,w)$\;

\For{each voter $n\in N$}{
Compute $p_n:=s_n-\sum_{c\in A\cap C_n: \ supp_w(c)< t'} w_{nc}$\;
Compute $q_n:=\sum_{c\in A\cap C_n: \ supp_w(c)\geq t'} w_{nc}/supp_w(c)$\;
}
\For{each candidate $c'\in C\setminus A$}{
Compute $linscore_w(c',t')=\frac{\sum_{n\in N_{c'}} p_n}{1+\sum_{n\in N_{c'}} q_n}$\;
}
Find a candidate $c_{\max}\in\arg\max_{c'\in C\setminus A} linscore_w(c',t')$\;
\Return $(c_{\max}, linscore_w(c_{\max},t'))$\;
 \caption{$\maxscore(A,w)$}
\end{algorithm}

\begin{lemma}\label{lem:maxscore}
For a partial solution $(A,w)$, Algorithm $\maxscore(A,w)$ runs in time $O(|E|\cdot \log k)$ and returns a tuple $(c_{\max}, t_{\max})$ such that $t_{\max}=score_w(c_{\max})=\max_{c'\in C\setminus A} score_w(c')$.
\end{lemma}
\begin{proof}
The correctness of the algorithm follows from the definition of linearized score and the arguments above. Each of the \textbf{for} loops executes in time $O(|E|)$ because in each one of them each edge is examined at most once. Thus, the running time is dominated by the call to function $\interval(A,w)$, which takes time $O(|E|\cdot \log k)$.
\end{proof}

This proves Lemma~\ref{lem:maxscore2}
We also highlight that the linearized score of a candidate $c'$ around the origin (i.e. setting $t'=0$) is such that %
%
$$\frac{1}{linscore_w(c',0)}=\frac{1+\sum_{n\in N_{c'}} \sum_{c\in A\cap C_n} \frac{w_{nc}}{supp_w(c)}}{\sum_{n\in N_{c'}} s_n},$$
%
which roughly corresponds to the \emph{load} candidate function being minimized in the $\phragmen$ heuristic (Algorithm~\ref{alg:phragmen}). Therefore, our heuristic can be seen as a generalization which, by considering further linearizations of the pre-score function, grants new candidates higher scores and thus higher supports. 

\section{Delayed proofs}\label{s:proofs}

\begin{proof}[Proof of Lemma~\ref{lem:lazybinary}]
$\lazy$ (Algorithm~\ref{alg:lazy}) clearly executes in time $O(B\cdot k)$. Hence, we focus on proving that the claimed number of iterations is enough, for a given parameter $\eps>0$.

Let $(A^*, w^*)$ be an optimal solution with support $t^*$. We start by finding bounds for $t^*$ via a pre-computation whose running time will be dominated by that of the main algorithm. For each candidate $c\in C$, we define its "potential support" as $psupp(c):=\sum_{n\in N_c} s_n$; clearly, it holds that $psupp(c)\geq supp_w^*(c)$ for any member $c\in A^*$. We find the potential supports of all candidates in $C$, and identify the $k$-th highest one, counting repetitions, which we denote by $psupp_k$. It follows that $psupp_k\geq supp_{w^*}(A^*)=t^*$. On the other hand, we also have that $t^* \geq psupp_k /k$, as we show now: Consider the solution $(A',w')$ where $A'$ contains the $k$ candidates in $C$ with highest potential support, and $w'$ is defined by $w'_{nc}:=s_n/|A\cap C_n| \geq s_n/k$ for each edge $nc$ with $c\in A\cap C_n$; it can then be checked that vector $w'$ is feasible, and $supp_w'(c)\geq psupp(c)/k$ for each $c\in A'$, so $t^*\geq supp_w'(A')\geq psupp_k/k$. Therefore, $psupp_k/k\leq t^*\leq psupp_k$.

Due to Lemma~\ref{lem:success}, to achieve a $(2+\eps)$-factor guarantee it suffices to find two target values $t'<t''$ such that $\lazy$ succeeds for $t'$ and fails for $t''$, and whose ratio is bounded by $t''/t'\leq 1+\eps/2$, and return the output for $t'$. We can find such target values via binary search, within the range $[t^*/2, t^*]\subseteq [psupp_k/(2k), psupp_k]$. If we initialize these variables to $t'=psupp_k/(2k)$ and $t''=psupp_k$, then their ratio starts at $2k$, and can be square-rooted at each iteration of the binary search by always testing their geometric mean. Hence, if it takes $r+1$ iterations to bring this ratio below $(1+\eps/2)$, then $(2k)^{1/2^r} > (1+\eps/2)$, so $r=O(\log(\eps^{-1}\log k))$. This completes the proof. 
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:Lebesgue}]
Recall that for any set $A\subseteq \mathbb{R}$, the indicator function $1_A:\mathbb{R}\rightarrow \mathbb{R}$ is defined as $1_A(t)=1$ if $t\in A$, and $0$ otherwise. For any $i\in I$, we can write
$$\alpha_i f(x_i) = \alpha_i \int_{0}^{f(x_i)} dt = \alpha_i\int_0^{\lim_{x\rightarrow \infty} f(x)} 1_{(-\infty, f(x_i)]}(t)dt,$$
and thus
\begin{align*}
    \sum_{i\in I} \alpha_i f(x_i) = \int_0^{\lim_{x\rightarrow \infty} f(x)} \Big(\sum_{i\in I} \alpha_i 1_{(-\infty, f(x_i)]}(t)\Big)dt = \int_0^{\lim_{x\rightarrow \infty} f(x)} \Big(\sum_{i\in I: \ f(x_i)\geq t} \alpha_i \Big)dt.
\end{align*}
This is a Lebesgue integral over the measure with weights $\alpha_i$. Now, conditions on function $f(x)$ are sufficient for its inverse $f^{-1}(t)$ to exist, with $f^{-1}(0)=\chi$. Substituting with the new variable $x=f^{-1}(t)$ on the formula above, where $t=f(x)$ and $dt=f'(x)dx$, we finally obtain
$$\sum_{i\in I} \alpha_i f(x_i) =\int_{\chi}^{\infty} \Big( \sum_{i\in I: \ x_i\geq x} \alpha_i \Big)(f'(x)dx).$$
\end{proof}

\section{Extra bits (to be removed or completed before publication)}

\begin{lemma}\label{lem:list}
Consider a list $L$ of $k$ real numbers, all within the range $[0,t]$ for some threshold $t>0$, and consider an iterative process that in each iteration: 
1) replaces the smallest entry in $L$ with the number $t$, and then 2) alters the entries in $L$ in such a way that neither the minimum nor the average value decreases.
For any $\eps>0$, we have that after at most $k\cdot (1+\ln (1/\eps))$ iterations the smallest entry in $L$ is at least $(1-\eps)\cdot t$.
\end{lemma}

\begin{proof}
Fix a constant $\eps>0$. Let $a^i$ and $m^i$ be respectively the average and the minimum value of list $L$ at the end of the $i$-th iteration, where $a^0$ and $m^0$ correspond to these values for the initial list $L$. 
We make two claims: first, that for $i\geq k\cdot \ln(1/\eps)$ we have that $a^i \geq (1-\eps) \cdot t$; 
second, that $m^{i+k-1}\geq a^i$ for any $i\geq 0$. Notice that the lemma easily follows from these two claims.

We start with the first claim. At each iteration $i\geq 1$, the first operation increases the average by an amount $(t - m^{i-1})/k$, and the second operation does not decrease the average. Hence, $a^i - a^{i-1}\geq (t-m^{i-1})/k \geq (t-a^{i-1})/k$. We thus obtain the following recursive inequality between $a^{i}$ and $a^{i-1}$:
\begin{align*}
a^i &\geq \frac{t}{k} + \Big( 1-\frac{1}{k} \Big) \cdot a^{i-1}\\
  &\geq \Big[ 1-\Big( 1-\frac{1}{k} \Big)^i \Big]\cdot t + \Big(1 - \frac{1}{k}\Big)^i\cdot a^0 & \text{(by induction on $i$)} \\
	&\geq \Big[ 1-\Big( 1-\frac{1}{k} \Big)^i \Big]\cdot t 
	 > \big( 1 - e^{-i/k} \big) \cdot t.
\end{align*}
Finally, it can be checked that if $i\geq k\cdot \ln (1/\eps)$, then $a^i > (1 - e^{i/k})\cdot t \geq (1-\eps)\cdot t$. 

We continue with the second claim. Fix an iteration $i\geq 0$. Our previous remark on the increase of the average value from one iteration to the next gives us 
$$a^{i+k}\geq a^{i+k-1} + \frac{t - m^{i+ k -1}}{k} \geq a^{i+ k - 2} + \frac{2t - m^{i+k-1} - m^{i+k-2}}{k} \geq \cdots \geq a^i + t - \frac{m^{i+k-1}+\cdots + m^i}{k}.$$ 
Notice now that neither of the two operations decreases the minimum value in the list, so the sequence of minimum values must be non-decreasing. 
In particular, we obtain that $(m^i + \cdots +m^{i+k-1})/k \leq m^{i+k-1}$, and the inequality above yields $a^{i+k}\geq a^i + t - m^{i+k-1}$.  
Finally, solving for $m^{i+k-1}$ we obtain that $m^{i+k-1}\geq a^i + t - a^{i+k}\geq a^i$, where we use the observation that the average must be upper bounded by $t$ in all iterations. This completes the proof.
\end{proof}

\begin{algorithm}[htb]\label{alg:unified}
\SetAlgoLined
\KwData{Balanced partial solution $(A,w)$ with $|A|\leq k$, error parameter $\eps>0$, Boolean $\balancing$.}

Let $\hat{t}\leftarrow \sum_{n\in N} s_n / |A|$\;
\While{True}{
  Let $(c_{\max},t_{\max})\leftarrow \maxscore(\bar{A},\bar{w})$\;
	\If{$|A|==k$}{
	  Find tuple $(c_{\min}, t_{\min})$ where $c_{\min}\in A$ and $t_{\min}=supp_w(c_{\min})=supp_w(A)$\; 
		\lIf {($t_{\min}$ \geq (1-\eps)\cdot t_{\max})} { \Return $(A,w)$ }
		Set $A\leftarrow A-c_{\min}$;%, and $w_{nc_{\min}}\leftarrow 0$ for each $n\in N_{c_{\min}}$;
	}
	\eIf{$\balancing$}{
	  Set $A\leftarrow A+c_{\max}$ and rebalance $w$ (replace $w$ with a balanced vector for $A$)\;
	}{
	  $(A,w)\leftarrow \ins(A,w,t_{\max})$\;
	}
}
\caption{Unified algorithm}
\end{algorithm}

\begin{lemma}
If the unified algorithm is given as input an arbitrary balanced partial solution $(A,w)$, an error parameter $\eps>0$, and a Boolean $\balancing$, it executes at most $k\cdot(1+\ln(1/\eps))$ iterations. 
\end{lemma}
\begin{proof}
Consider the list of numbers $\{\min\}$
\end{proof}