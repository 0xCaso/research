
\section{Delayed proofs}\label{s:proofs}

\begin{proof}[Proof of Lemma~\ref{lem:lazybinary}]
$\lazy$ (Algorithm~\ref{alg:lazy}) clearly executes in time $O(B\cdot k)$. Hence, we focus on proving that the claimed number of iterations is enough, for a given parameter $\eps>0$.

Let $(A^*, w^*)$ be an optimal solution with support $t^*$. We start by finding bounds for $t^*$ via a pre-computation whose running time will be dominated by that of the main algorithm. For each candidate $c\in C$, we define its "potential support" as $psupp(c):=\sum_{n\in N_c} s_n$; clearly, it holds that $psupp(c)\geq supp_w^*(c)$ for any member $c\in A^*$. We find the potential supports of all candidates in $C$, and identify the $k$-th highest one, counting repetitions, which we denote by $psupp_k$. It follows that $psupp_k\geq supp_{w^*}(A^*)=t^*$. On the other hand, we also have that $t^* \geq psupp_k /k$, as we show now: Consider the solution $(A',w')$ where $A'$ contains the $k$ candidates in $C$ with highest potential support, and $w'$ is defined by $w'_{nc}:=s_n/|A\cap C_n| \geq s_n/k$ for each edge $nc$ with $c\in A\cap C_n$; it can then be checked that vector $w'$ is feasible, and $supp_w'(c)\geq psupp(c)/k$ for each $c\in A'$, so $t^*\geq supp_w'(A')\geq psupp_k/k$. Therefore, $psupp_k/k\leq t^*\leq psupp_k$.

Due to Lemma~\ref{lem:success}, to achieve a $(2+\eps)$-factor guarantee it suffices to find two target values $t'<t''$ such that $\lazy$ succeeds for $t'$ and fails for $t''$, and whose ratio is bounded by $t''/t'\leq 1+\eps/2$, and return the output for $t'$. We can find such target values via binary search, within the range $[t^*/2, t^*]\subseteq [psupp_k/(2k), psupp_k]$. If we initialize these variables to $t'=psupp_k/(2k)$ and $t''=psupp_k$, then their ratio starts at $2k$, and can be square-rooted at each iteration of the binary search by always testing their geometric mean. Hence, if it takes $r+1$ iterations to bring this ratio below $(1+\eps/2)$, then $(2k)^{1/2^r} > (1+\eps/2)$, so $r=O(\log(\eps^{-1}\log k))$. This completes the proof. 
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:Lebesgue}]
Recall that for any set $A\subseteq \mathbb{R}$, the indicator function $1_A:\mathbb{R}\rightarrow \mathbb{R}$ is defined as $1_A(t)=1$ if $t\in A$, and $0$ otherwise. For any $i\in I$, we can write
$$\alpha_i f(x_i) = \alpha_i \int_{0}^{f(x_i)} dt = \alpha_i\int_0^{\lim_{x\rightarrow \infty} f(x)} 1_{(-\infty, f(x_i)]}(t)dt,$$
and thus
\begin{align*}
    \sum_{i\in I} \alpha_i f(x_i) = \int_0^{\lim_{x\rightarrow \infty} f(x)} \Big(\sum_{i\in I} \alpha_i 1_{(-\infty, f(x_i)]}(t)\Big)dt = \int_0^{\lim_{x\rightarrow \infty} f(x)} \Big(\sum_{i\in I: \ f(x_i)\geq t} \alpha_i \Big)dt.
\end{align*}
This is a Lebesgue integral over the measure with weights $\alpha_i$. Now, conditions on function $f(x)$ are sufficient for its inverse $f^{-1}(t)$ to exist, with $f^{-1}(0)=\chi$. Substituting with the new variable $x=f^{-1}(t)$ on the formula above, where $t=f(x)$ and $dt=f'(x)dx$, we finally obtain
$$\sum_{i\in I} \alpha_i f(x_i) =\int_{\chi}^{\infty} \Big( \sum_{i\in I: \ x_i\geq x} \alpha_i \Big)(f'(x)dx).$$
\end{proof}

\section{Extra bits (to be removed or completed before publication)}

\begin{lemma}\label{lem:list}
Consider a list $L$ of $k$ real numbers, all within the range $[0,t]$ for some threshold $t>0$, and consider an iterative process that in each iteration: 
1) replaces the smallest entry in $L$ with the number $t$, and then 2) alters the entries in $L$ in such a way that neither the minimum nor the average value decreases.
For any $\eps>0$, we have that after at most $k\cdot (1+\ln (1/\eps))$ iterations the smallest entry in $L$ is at least $(1-\eps)\cdot t$.
\end{lemma}

\begin{proof}
Fix a constant $\eps>0$. Let $a^i$ and $m^i$ be respectively the average and the minimum value of list $L$ at the end of the $i$-th iteration, where $a^0$ and $m^0$ correspond to these values for the initial list $L$. 
We make two claims: first, that for $i\geq k\cdot \ln(1/\eps)$ we have that $a^i \geq (1-\eps) \cdot t$; 
second, that $m^{i+k-1}\geq a^i$ for any $i\geq 0$. Notice that the lemma easily follows from these two claims.

We start with the first claim. At each iteration $i\geq 1$, the first operation increases the average by an amount $(t - m^{i-1})/k$, and the second operation does not decrease the average. Hence, $a^i - a^{i-1}\geq (t-m^{i-1})/k \geq (t-a^{i-1})/k$. We thus obtain the following recursive inequality between $a^{i}$ and $a^{i-1}$:
\begin{align*}
a^i &\geq \frac{t}{k} + \Big( 1-\frac{1}{k} \Big) \cdot a^{i-1}\\
  &\geq \Big[ 1-\Big( 1-\frac{1}{k} \Big)^i \Big]\cdot t + \Big(1 - \frac{1}{k}\Big)^i\cdot a^0 & \text{(by induction on $i$)} \\
	&\geq \Big[ 1-\Big( 1-\frac{1}{k} \Big)^i \Big]\cdot t 
	 > \big( 1 - e^{-i/k} \big) \cdot t.
\end{align*}
Finally, it can be checked that if $i\geq k\cdot \ln (1/\eps)$, then $a^i > (1 - e^{i/k})\cdot t \geq (1-\eps)\cdot t$. 

We continue with the second claim. Fix an iteration $i\geq 0$. Our previous remark on the increase of the average value from one iteration to the next gives us 
$$a^{i+k}\geq a^{i+k-1} + \frac{t - m^{i+ k -1}}{k} \geq a^{i+ k - 2} + \frac{2t - m^{i+k-1} - m^{i+k-2}}{k} \geq \cdots \geq a^i + t - \frac{m^{i+k-1}+\cdots + m^i}{k}.$$ 
Notice now that neither of the two operations decreases the minimum value in the list, so the sequence of minimum values must be non-decreasing. 
In particular, we obtain that $(m^i + \cdots +m^{i+k-1})/k \leq m^{i+k-1}$, and the inequality above yields $a^{i+k}\geq a^i + t - m^{i+k-1}$.  
Finally, solving for $m^{i+k-1}$ we obtain that $m^{i+k-1}\geq a^i + t - a^{i+k}\geq a^i$, where we use the observation that the average must be upper bounded by $t$ in all iterations. This completes the proof.
\end{proof}

\begin{algorithm}[htb]\label{alg:unified}
\SetAlgoLined
\KwData{Balanced partial solution $(A,w)$ with $|A|\leq k$, error parameter $\eps>0$, Boolean $\balancing$.}

Let $\hat{t}\leftarrow \sum_{n\in N} s_n / |A|$\;
\While{True}{
  Let $(c_{\max},t_{\max})\leftarrow \maxscore(\bar{A},\bar{w})$\;
	\If{$|A|==k$}{
	  Find tuple $(c_{\min}, t_{\min})$ where $c_{\min}\in A$ and $t_{\min}=supp_w(c_{\min})=supp_w(A)$\; 
		\lIf {($t_{\min} \geq (1-\eps)\cdot t_{\max}$)} { \Return $(A,w)$ }
		Set $A\leftarrow A-c_{\min}$;%, and $w_{nc_{\min}}\leftarrow 0$ for each $n\in N_{c_{\min}}$;
	}
	\eIf{$\balancing$}{
	  Set $A\leftarrow A+c_{\max}$ and rebalance $w$ (replace $w$ with a balanced vector for $A$)\;
	}{
	  $(A,w)\leftarrow \ins(A,w,t_{\max})$\;
	}
}
\caption{Unified algorithm}
\end{algorithm}

\begin{lemma}
If the unified algorithm is given as input an arbitrary balanced partial solution $(A,w)$, an error parameter $\eps>0$, and a Boolean $\balancing$, it executes at most $k\cdot(1+\ln(1/\eps))$ iterations. 
\end{lemma}
\begin{proof}
Consider the list of numbers $\{\min\}$
\end{proof}