\subsection{Networking}\label{sec:networking}

In the above sections we talk abstractly about nodes sending data to another
node or other set of nodes, without being too specific as to how this is
achieved. We do this to simplify the model and to clearly delineate a
separation of concerns between different layers. Of course, in a real-world
decentralised system the networking part also has to be decentralised - it is
no good if all communication passes through a few central servers, even if the
protocol running on top of it is decentralised within itself. \footnote{As a
concrete example on why this is the case: in certain security models, including
the traditional Byzantine fault-tolerant setting, nodes are modelled as capable
of being malicious but no consideration is given to malicious edges. Under
these models, if an edge is malicous in practice in the real-world, it is the
corresponding node(s) that is treated as malicious in the model. If the
underlying communications network is centralised, this effectively gives the
central parties the ability to "corrupt" a large number (e.g. $> 1/3$) of nodes
within this model, breaking its security, even if they don't actually have
arbitrary execution rights on that many nodes.} In this section we outline and
enumerate the specific sending primitives that we require in Polkadot, and
sketch a high-level design on how we achieve these in a decentralised way, with
the specifics to be refined as we move forward with a production system.

Polkadot needs networking for a number of its components as follows.
\begin{enumerate}
\item accepting transactions from users for (a) parachains and (b) the relay chain
\item collation of blocks within a parachain
\item validation of parachain blocks, via proofs and attestation, and making these efficiently available \ref{sec:validity-and-availability}
\item distributing relay chain blocks \ref{sec:relaychainblockproduction} and votes \ref{sec:grandpa}
\item sending messages between different parachains \ref{sec:ICMP}
\item synchronising new state for (a) parachains and (b) the relay chain
\end{enumerate}

1(a), 2 and 3(a) are strictly outside of the scope of Polkadot, being entirely chosen by each parachain for themselves, but schemes similar to the ones described below (to be used by the relay chain) may also be used by parachains if they decide they are suitable.

\subsubsection{Message flows}

Generally speaking, the communication data flow follows these patterns:

\begin{itemize}
\item messages within a parachain and within the relay chain are gossipped
\item messages between a parachain and the relay chain are sent to some non-specific targets, e.g. a parachain validators to some collators, or a collator to some parachain validators. Gossip within the target set then propagates the messages to the entire target set.
\item messages from users to submit transactions are sent to some non-specific targets, e.g. collators or validators
\item as a special case, the erasure coded scheme for availability is sent directly from parachain validators to specific other validators. The scheme is design specifically to handle unreliability in the direct-sending primitive, which is generally harder to achieve reliability for than gossip or non-specific sending to a subset.
\end{itemize}

\subsubsection{Authentication and discovery}

For node discovery we use a similar networking scheme as many other blockchains do, that is using the widely used distributed hash table (DHT), Kademlia \cite{Maymounkov:2002:Kademila}. Kademlia is a DHT that uses XOR distance metric for finding a node and is often used for networks with high churn. We use Protocols Labs' libp2p libraries \cite{} Kademlia implementation with some changes for this purpose.

\subsubsection{Gossiping}
The main idea of gossiping is to broadcast every newly received message in the nodes local network (peers that the node is aware of). Moreover, we apply some restriction to the gossiping protocol to prevent bandwidth problems, e.g., denial-of-service (DoS-ed) as follows.

For GRANDPA we only allow two votes being received for each type of vote, round number, and voter. Any further votes will be ignored. For block production only valid block producers are allowed to produce one block per round and further blocks will be ignored.

We use \emph{Sentry nodes} that are proxy servers who receive all the traffic that would go to a certain validator and forward the traffic as soon as the validator is able to receive it.

To prevent Eclipse attacks \cite{} we allow for routing tables that are large enough to contain at least some honest nodes.

\subsubsection{Direct Routing}
The aim of direct routing is only to send messages to the intended receiver.
This is saving lots of traffic and is useful when messages only have one receiver and do not need to reach more than one validator such as in the case of erasure coded pieces.

\subsubsection{Interchain Messaging}
If the sending parachain and the receiving parachain have common honest full nodes then the message will be gossiped from without issue.
However, if parachain validators of the receiving parachain realize that the message has not arrived they request the message from the parachain validator of the sending parachain.
Once, they receive it they gossip it in the receiving parachain.

\subsubsection{Message types}
Below we give an overview of where and how each type of message is sent. The column \emph{Nets} refers to the networks where a type of message is traversing and the column \emph{Mode} refers to the type of  routing. The column \emph{Static DHT Prefixes} refers to the DHT prefixes of the receivers if we use a one DHT for all and use prefixes to separate sub-networks.

We use gossiping mainly when the message type is small. For example, GRANDPA votes and attestation are very small. For bigger data structures we need to either use bloom filters or use direct routing.

\textbf{Nets}:

$PC$ = Parachain Collator and parachain full nodes

$PV$= Parachain Validators

$V$ = Validator and relay chain full nodes (->Validator Network ID on chain)

\textbf{Mode}:

$D$ = Direct transfer

$G$ = Gossip

$B$ = Big / Bloomfiltered

$R$ = Receving e.g., $PC_{R}$ refers to the receiving parachain's collators and full nodes

$S$ = Sending e.g., $PC_{S}$ refers to the sending parachain's collators and full nodes

\begin{table}[h]
\begin{tabular}{lllll}
& \textbf{Message type}  & \textbf{Nets}  & \textbf{Mode}  & \textbf{Static DHT Prefixes} \\
\hline
& Parachain TXs  & $PC$   & $G$   &Depends on Parachain\\
& PoV block  & $PC$ + $PV$  & $D$         & - \\
& Parachain Block & $PC$ + $PV$   & $G$:$PC$, $D$:$PV$ &$P_0$,...,$P_n$\\
& Attestations   & $V$   & $G$  & $V$ \\
& Relay chain TXs & $V$    & $G$   & $V$ \\
& Relay chain block & $PC$ + $V$  & $G^B$   & General \\
& Messages  & $PC_{R + S}$ & $G^*$     & V \\
& Erasure coded    & $V$           & $G$ (D later)        &$V$\\
& GRANDPA Votes   &$V$          & $G$      &$V$\\
\end{tabular}
\end{table}

* fallback->D:$PV_{R}$ request $PV_{S}$ and then uses G at $PC_{R}$ to spread them,
second fallback->D: $PV_{R}$ recover messages from erasure codes obtained from V and use G at $PC_{R}$ to spread them.
