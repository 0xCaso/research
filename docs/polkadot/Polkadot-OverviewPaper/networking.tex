\subsection{Networking}\label{sec:networking}

Networking is an essential part of decentralized technologies.
Polkadot needs networking for a number of its components as follows.
\begin{itemize}
\item distributing votes for the consensus protocol GRANDPA \ref{sec:grandpa}
\item distributing relay chain blocks\ref{sec:relaychainblockproduction}, parachain blocks, and PoV blocks
\item receiving parachain blocks that have been produced by parachains
\item distributing and recovering of erasure coded pieces for the availability and validity scheme \ref{sec:validity-and-availability}
\item distributing attestations for the availability and validity scheme \ref{sec:validity-and-availability}
\item sending messages from a parachain to another parachain for the interchain messaging scheme \ref{sec:ICMP}
\end{itemize}

We use \emph{gossiping} for all networking use cases mentioned above, except for distributing and recovery of erasure coded pieces for the availability and validity mechanism, where we use \emph{direct routing} for scalability.
For interchain messaging we use first gossiping with a fall back to direct routing if gossiping fails.

We will not discuss the networking details of parachain block production, since it is out of scope for Polkadot.

We refer to all the entites as nodes when describing networking.


\subsubsection{Gossiping}
The main idea of gossiping is to broadcast every newly received message in the nodes local network (peers that the node is aware of).
For node discovery we use a similar networking scheme as many other blockchains do that is using the widely used distributed hast table (DHT), Kademlia \cite{Maymounkov:2002:Kademila}.
Kademlia is a DHT that uses XOR distance metric for finding a node and is often used for networks with high churn.
We use Protocols Labs' libp2p libraries \cite{} Kademlia implementation with some changes for this purpose.

Moreover, we apply some restriction to the gossiping protocol to prevent bandwidth problems, e.g., denial-of-service (DoS-ed) as follows.

For GRANDPA we only allow two votes being received for each type of vote, round number, and voter. Any further votes will be ignored.
For block production only valid block producers are allowed to produce one block per round and further blocks will be ignored.

We use \emph{Sentry nodes} that are proxy servers who receive all the traffic that would go to a certain validator and forward the traffic as soon as the validator is able to receive it.

To prevent Eclipsing attacks \cite{} we allow for routing tables that are large enough to contain at least some honest nodes.

\subsubsection{Direct Routing}
The aim of direct routing is only to send messages to the intended receiver.
This is saving lots of traffic and is useful when messages only have one receiver and do not need to reach more than one validator such as in the case of erasure coded pieces.

\subsubsection{Interchain Messaging}
If the sending parachain and the receiving parachain have common honest full nodes then the message will be gossiped from without issue.
However, if parachain validators of the receiving parachain realize that the message has not arrived they request the message from the parachain validator of the sending parachain.
Once, they receive it they gossip it in the receiving parachain.


Below we give an overview of where and how each type of message is sent. The column *Nets* refers to the networks where a type of message is traversing and the column *Mode* refers to the type of  routing. The column *Static DHT Prefixes* refers to the DHT prefixes of the receivers if we use a one DHT for all and use prefixes to separate sub-networks.

We use gossiping mainly when the message type is small. For example, GRANDPA votes and attestation are very small. For bigger data structures we need to either use bloom filters or use direct routing.

**Nets**:

$PC$ = Parachain Collator and parachain full nodes

$PV$= Parachain Validators

$V$ = Validator and relay chain full nodes (->Validator Network ID on chain)

**Mode**:

$D$ = Direct transfer

$G$ = Gossip

$B$ = Big / Bloomfiltered

$R$ = Receving e.g., $PC_{R}$ refers to the receiving parachain's collators and full nodes

$S$ = Sending e.g., $PC_{S}$ refers to the sending parachain's collators and full nodes

\begin{table}[]
\begin{tabular}{lllll}
& Message type  & Nets  & Mode  & Static DHT Prefixes \\
& Parachain TXs  & $PC$   & $G$   &Depends on Parachain\\
& PoV block  & $PC$ + $PV$  & $D$         & - \\
& Parachain Block & $PC$ + $PV$   & $G$:$PC$, $D$:$PV$ &$P_0$,...,$P_n$\\
& Attestations   & $V$   & $G$  & $V$ \\
& Relay chain TXs & $V$    & $G$   & $V$ \\
& Relay chain block & $PC$ + $V$  & $G^B$   & General \\
& Messages  & $PC_{R + S}$ & $G^*$     & V \\
& Erasure coded    & $V$           & $G$ (D later)        &$V$\\
& GRANDPA Votes   &$V$          & $G$      &$V$\\
\end{tabular}
\end{table}

* fallback->D:$PV_{R}$ request $PV_{S}$ and then uses G at $PC_{R}$ to spread them,
second fallback->D: $PV_{R}$ recover messages from erasure codes obtained from V and use G at $PC_{R}$ to spread them.
